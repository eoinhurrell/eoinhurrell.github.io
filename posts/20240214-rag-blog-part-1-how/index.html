<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Retrieval-Augmented Generation Experiments (Part 1) | </title><meta name=keywords content="post"><meta name=description content="I&rsquo;ve been researching LLMs, trying to find practical ways I can use them for myself, in my work or at home. One area that interests me is as tool to better understand the notes I&rsquo;ve taken, topics I frequently learn about and areas I might want to explore more. I&rsquo;m not the first to explore this territory (https://x.com/fortelabs/status/1749861644245848361).
I&rsquo;ll be looking at using a local model and building the system myself. I&rsquo;m going to detail how I&rsquo;ve built a toy system to chat with my notes. I&rsquo;ll be using Ollama1 to serve an open-source model locally, and building the application using LangChain2 and StreamLit3."><meta name=author content="Eoin H"><link rel=canonical href=https://www.eoinhurrell.com/posts/20240214-rag-blog-part-1-how/><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://www.eoinhurrell.com/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://www.eoinhurrell.com/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://www.eoinhurrell.com/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://www.eoinhurrell.com/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://www.eoinhurrell.com/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://www.eoinhurrell.com/posts/20240214-rag-blog-part-1-how/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://www.eoinhurrell.com/posts/20240214-rag-blog-part-1-how/"><meta property="og:title" content="Retrieval-Augmented Generation Experiments (Part 1)"><meta property="og:description" content="I’ve been researching LLMs, trying to find practical ways I can use them for myself, in my work or at home. One area that interests me is as tool to better understand the notes I’ve taken, topics I frequently learn about and areas I might want to explore more. I’m not the first to explore this territory (https://x.com/fortelabs/status/1749861644245848361).
I’ll be looking at using a local model and building the system myself. I’m going to detail how I’ve built a toy system to chat with my notes. I’ll be using Ollama1 to serve an open-source model locally, and building the application using LangChain2 and StreamLit3."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-14T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-14T00:00:00+00:00"><meta property="article:tag" content="Post"><meta name=twitter:card content="summary"><meta name=twitter:title content="Retrieval-Augmented Generation Experiments (Part 1)"><meta name=twitter:description content="I&rsquo;ve been researching LLMs, trying to find practical ways I can use them for myself, in my work or at home. One area that interests me is as tool to better understand the notes I&rsquo;ve taken, topics I frequently learn about and areas I might want to explore more. I&rsquo;m not the first to explore this territory (https://x.com/fortelabs/status/1749861644245848361).
I&rsquo;ll be looking at using a local model and building the system myself. I&rsquo;m going to detail how I&rsquo;ve built a toy system to chat with my notes. I&rsquo;ll be using Ollama1 to serve an open-source model locally, and building the application using LangChain2 and StreamLit3."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Retrieval-Augmented Generation Experiments (Part 1)","item":"https://www.eoinhurrell.com/posts/20240214-rag-blog-part-1-how/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Retrieval-Augmented Generation Experiments (Part 1)","name":"Retrieval-Augmented Generation Experiments (Part 1)","description":"I\u0026rsquo;ve been researching LLMs, trying to find practical ways I can use them for myself, in my work or at home. One area that interests me is as tool to better understand the notes I\u0026rsquo;ve taken, topics I frequently learn about and areas I might want to explore more. I\u0026rsquo;m not the first to explore this territory (https://x.com/fortelabs/status/1749861644245848361).\nI\u0026rsquo;ll be looking at using a local model and building the system myself. I\u0026rsquo;m going to detail how I\u0026rsquo;ve built a toy system to chat with my notes. I\u0026rsquo;ll be using Ollama1 to serve an open-source model locally, and building the application using LangChain2 and StreamLit3.\n","keywords":["post"],"articleBody":"I’ve been researching LLMs, trying to find practical ways I can use them for myself, in my work or at home. One area that interests me is as tool to better understand the notes I’ve taken, topics I frequently learn about and areas I might want to explore more. I’m not the first to explore this territory (https://x.com/fortelabs/status/1749861644245848361).\nI’ll be looking at using a local model and building the system myself. I’m going to detail how I’ve built a toy system to chat with my notes. I’ll be using Ollama1 to serve an open-source model locally, and building the application using LangChain2 and StreamLit3.\nThis will end with an interface I can use to ask questions that an LLM will attempt to answer, using (and even citing) notes I’ve made in my past reading. I hope to use this to assist in my learning on a number of topics I’m interested in, and in future posts I may evaluate how successfully this system aids in that task.\nRetrieval-Augmented Generation Retrieval-Augmented Generation (RAG) is a revolutionary concept in conversational AI that combines data retrieval and generation models to create more comprehensive and accurate responses. Unlike traditional models relying solely on internal knowledge, RAG systems utilize external information from a pre-built index, enhancing the overall conversational experience. RAG will form the basis of my approach here, as it is a quick and easy way to provide context not available to the model during training. Alternatives would be either fine-tuning or training a model from scratch.\nSemantic Search Semantic search is an essential component in building such an index. It uses vectors, mathematical representations of data points, to measure similarity between query and documents in a high-dimensional space. By converting textual information into numerical vectors, semantic search algorithms can efficiently analyze large datasets and retrieve the most relevant pieces of information.\nIn prior roles I have built semantic search using tools like ElasticSearch4, Spotify’s annoy KNN library5 and similar tools. Vector store tooling has developed hugely as a result of the importance of embeddings in LLM work, so there’s much less friction to throwing something together for a prototype. To get started, let’s build our index:\n#!/usr/bin/env python3 from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_community.vectorstores import FAISS from langchain_community.embeddings import OllamaEmbeddings from langchain_community.document_loaders import ObsidianLoader loader = ObsidianLoader(\"\") docs = loader.load() # print(docs[^0]) text_splitter = RecursiveCharacterTextSplitter() documents = text_splitter.split_documents(docs) vectorstore = FAISS.from_documents(documents, embedding=OllamaEmbeddings()) vectorstore.save_local(\"faiss_index\") This will create a local directory named ‘faiss_index’ that contains the vector store.\nStreamLit chatbot interface Now that we have our index, let’s explore how a RAG chatbot would work: When a user interacts with the bot, their query is first processed by a semantic search algorithm to find the most relevant documents from the index. These documents are then fed into a generation model along with the user’s query to produce a response.\nI’ve wrapped this flow into a StreamLit interface. This is adapted from a StreamLit tutorial on writing an LLM bot4, mostly to use Ollama. I’ve also added memory to the chatbot, so it will remember the line of questioning. I’m including all of the code here, as it’s short and straightforward. Let’s engage in conversation with our RAG-assisted chatbot using the index we built earlier:\n#!/usr/bin/env python3 from langchain_community.chat_models import ChatOllama from langchain_community.embeddings import OllamaEmbeddings from langchain.prompts import PromptTemplate from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain.schema import format_document from langchain_core.messages import get_buffer_string from langchain_core.runnables import RunnableLambda, RunnablePassthrough from langchain_community.vectorstores import FAISS from operator import itemgetter from langchain.memory import ConversationBufferMemory import streamlit as st vectorstore = FAISS.load_local( \"faiss_index\", embeddings=OllamaEmbeddings()) retriever = vectorstore.as_retriever() chat = ChatOllama(model=\"mistral\") memory = ConversationBufferMemory( return_messages=True, output_key=\"answer\", input_key=\"question\" ) # First we add a step to load memory # This adds a \"memory\" key to the input object loaded_memory = RunnablePassthrough.assign( chat_history=RunnableLambda( memory.load_memory_variables) | itemgetter(\"history\"), ) DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template( template=\"{page_content}\") def _combine_documents( docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\" ): doc_strings = [format_document(doc, document_prompt) for doc in docs] return document_separator.join(doc_strings) _template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language. Chat History: {chat_history} Follow Up Input: {question} Standalone question:\"\"\" CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template) template = \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\" ANSWER_PROMPT = ChatPromptTemplate.from_template(template) # Now we calculate the standalone question standalone_question = { \"standalone_question\": { \"question\": lambda x: x[\"question\"], \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]), } | CONDENSE_QUESTION_PROMPT | chat | StrOutputParser(), } # Now we retrieve the documents retrieved_documents = { \"docs\": itemgetter(\"standalone_question\") | retriever, \"question\": lambda x: x[\"standalone_question\"], } # Now we construct the inputs for the final prompt final_inputs = { \"context\": lambda x: _combine_documents(x[\"docs\"]), \"question\": itemgetter(\"question\"), } # And finally, we do the part that returns the answers answer = { \"answer\": final_inputs | ANSWER_PROMPT | chat, \"docs\": itemgetter(\"docs\"), } # And now we put it all together! final_chain = loaded_memory | standalone_question | retrieved_documents | answer st.title(\"NoteChat\") # Set a default model if \"model\" not in st.session_state: st.session_state[\"model\"] = final_chain # Initialize chat history if \"memory\" not in st.session_state: st.session_state[\"memory\"] = memory if \"messages\" not in st.session_state: st.session_state[\"messages\"] = [] # Display chat messages from history on app rerun for message in st.session_state.messages: with st.chat_message(message[\"role\"]): st.markdown(message[\"content\"]) # Accept user input if prompt := st.chat_input(\"What is up?\"): # Add user message to chat history st.session_state.messages.append({\"role\": \"user\", \"content\": prompt}) # Display user message in chat message container with st.chat_message(\"user\"): st.markdown(prompt) # Display assistant response in chat message container with st.chat_message(\"assistant\"): message_placeholder = st.empty() full_response = \"\" response = final_chain.invoke({\"question\": prompt}) memory.save_context({\"question\": prompt}, { \"answer\": response[\"answer\"].content}) full_response = f\"\"\"{response['answer'].content}\"\"\" # for doc in response[\"answer\"][\"doc\"]: # full_response += f\"\\n- {doc['path']}\" full_response += \"\\n\" st.session_state.messages.append( {\"role\": \"assistant\", \"content\": full_response}) Conclusion The combination of RAG and semantic search using vector-based indexing allows conversational AI systems to deliver more accurate and comprehensive responses. With open-source tools like LangChain and Ollama, developers can easily experiment with these technologies and push the boundaries of conversational AI applications.\nAs a preliminary qualitative evaluation, it has been interesting to explore how an LLM ‘understands’ my notes. I’m impressed by how it will draw in contrasting ideas I wouldn’t have thought of, but it also has issues, it frequently gets confused and doesn’t respond as I would expect, potentially because the corpus of my notes is very diverse, it includes highlights on anything I have read. I’ve seen the LLM respond as a medieval lord, because it pulled in a note from a fantasy book when I asked a philosophical question. Further work is needed to tune the prompt I’m using. I’d also like to find ways to do more quantitative evaluation of the results returned, even though this is only a toy side project.\nFootnotes Ollama is a great open-source local LLM server https://ollama.com ↩︎\nLangChain is an open-source framework designed for developing applications using LLMs. https://langchain.ai ↩︎\nStreamLit is a powerful way to turn data scripts into shareable web apps https://streamlit.io ↩︎\nhttps://www.elastic.co/elasticsearch, or the open alternative https://opensearch.org ↩︎ ↩︎\nhttps://github.com/spotify/annoy ↩︎\n","wordCount":"1159","inLanguage":"en","datePublished":"2024-02-14T00:00:00Z","dateModified":"2024-02-14T00:00:00Z","author":[{"@type":"Person","name":"Eoin H"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.eoinhurrell.com/posts/20240214-rag-blog-part-1-how/"},"publisher":{"@type":"Organization","name":"","logo":{"@type":"ImageObject","url":"https://www.eoinhurrell.com/%3Clink%20/%20abs%20url%3E"}}}</script><script>!function(e,t){var n,s,o,i;t.__SV||(window.posthog=t,t._i=[],t.init=function(a,r,c){function d(e,t){var n=t.split(".");2==n.length&&(e=e[n[0]],t=n[1]),e[t]=function(){e.push([t].concat(Array.prototype.slice.call(arguments,0)))}}(n=e.createElement("script")).type="text/javascript",n.crossOrigin="anonymous",n.async=!0,n.src=r.api_host.replace(".i.posthog.com","-assets.i.posthog.com")+"/static/array.js",(i=e.getElementsByTagName("script")[0]).parentNode.insertBefore(n,i);var l=t;for(void 0!==c?l=t[c]=[]:c="posthog",l.people=l.people||[],l.toString=function(e){var t="posthog";return"posthog"!==c&&(t+="."+c),e||(t+=" (stub)"),t},l.people.toString=function(){return l.toString(1)+".people (stub)"},o="init capture register register_once register_for_session unregister unregister_for_session getFeatureFlag getFeatureFlagPayload isFeatureEnabled reloadFeatureFlags updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures on onFeatureFlags onSessionId getSurveys getActiveMatchingSurveys renderSurvey canRenderSurvey identify setPersonProperties group resetGroups setPersonPropertiesForFlags resetPersonPropertiesForFlags setGroupPropertiesForFlags resetGroupPropertiesForFlags reset get_distinct_id getGroups get_session_id get_session_replay_url alias set_config startSessionRecording stopSessionRecording sessionRecordingStarted captureException loadToolbar get_property getSessionProperty createPersonProfile opt_in_capturing opt_out_capturing has_opted_in_capturing has_opted_out_capturing clear_opt_in_out_capturing debug getPageViewId captureTraceFeedback captureTraceMetric".split(" "),s=0;s<o.length;s++)d(l,o[s]);t._i.push([a,r,c])},t.__SV=1)}(document,window.posthog||[]),posthog.init("",{api_host:"https://us.i.posthog.com",person_profiles:"identified_only"})</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><div class=logo-switches></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://www.eoinhurrell.com/>Home</a></div><h1 class="post-title entry-hint-parent">Retrieval-Augmented Generation Experiments (Part 1)</h1><div class=post-meta><span title='2024-02-14 00:00:00 +0000 UTC'>February 14, 2024</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Eoin H</div></header><div class=post-content><p>I&rsquo;ve been researching LLMs, trying to find practical ways I can use them for myself, in my work or at home. One area that interests me is as tool to better understand the notes I&rsquo;ve taken, topics I frequently learn about and areas I might want to explore more. I&rsquo;m not the first to explore this territory (<a href=https://x.com/fortelabs/status/1749861644245848361)>https://x.com/fortelabs/status/1749861644245848361)</a>.</p><p>I&rsquo;ll be looking at using a local model and building the system myself. I&rsquo;m going to detail how I&rsquo;ve built a toy system to chat with my notes. I&rsquo;ll be using Ollama<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> to serve an open-source model locally, and building the application using LangChain<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> and StreamLit<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</p><p>This will end with an interface I can use to ask questions that an LLM will attempt to answer, using (and even citing) notes I&rsquo;ve made in my past reading. I hope to use this to assist in my learning on a number of topics I&rsquo;m interested in, and in future posts I may evaluate how successfully this system aids in that task.</p><h1 id=retrieval-augmented-generation>Retrieval-Augmented Generation<a hidden class=anchor aria-hidden=true href=#retrieval-augmented-generation>#</a></h1><p>Retrieval-Augmented Generation (RAG) is a revolutionary concept in conversational AI that combines data retrieval and generation models to create more comprehensive and accurate responses. Unlike traditional models relying solely on internal knowledge, RAG systems utilize external information from a pre-built index, enhancing the overall conversational experience. RAG will form the basis of my approach here, as it is a quick and easy way to provide context not available to the model during training. Alternatives would be either fine-tuning or training a model from scratch.</p><h1 id=semantic-search>Semantic Search<a hidden class=anchor aria-hidden=true href=#semantic-search>#</a></h1><p>Semantic search is an essential component in building such an index. It uses vectors, mathematical representations of data points, to measure similarity between query and documents in a high-dimensional space. By converting textual information into numerical vectors, semantic search algorithms can efficiently analyze large datasets and retrieve the most relevant pieces of information.</p><p>In prior roles I have built semantic search using tools like ElasticSearch<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>, Spotify&rsquo;s annoy KNN library<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> and similar tools. Vector store tooling has developed hugely as a result of the importance of embeddings in LLM work, so there&rsquo;s much less friction to throwing something together for a prototype. To get started, let&rsquo;s build our index:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python3</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.text_splitter <span style=color:#f92672>import</span> RecursiveCharacterTextSplitter
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.vectorstores <span style=color:#f92672>import</span> FAISS
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.embeddings <span style=color:#f92672>import</span> OllamaEmbeddings
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.document_loaders <span style=color:#f92672>import</span> ObsidianLoader
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loader <span style=color:#f92672>=</span> ObsidianLoader(<span style=color:#e6db74>&#34;&lt;path to obsidian vault&gt;&#34;</span>)
</span></span><span style=display:flex><span>docs <span style=color:#f92672>=</span> loader<span style=color:#f92672>.</span>load()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># print(docs[^0])</span>
</span></span><span style=display:flex><span>text_splitter <span style=color:#f92672>=</span> RecursiveCharacterTextSplitter()
</span></span><span style=display:flex><span>documents <span style=color:#f92672>=</span> text_splitter<span style=color:#f92672>.</span>split_documents(docs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>vectorstore <span style=color:#f92672>=</span> FAISS<span style=color:#f92672>.</span>from_documents(documents, embedding<span style=color:#f92672>=</span>OllamaEmbeddings())
</span></span><span style=display:flex><span>vectorstore<span style=color:#f92672>.</span>save_local(<span style=color:#e6db74>&#34;faiss_index&#34;</span>)
</span></span></code></pre></div><p>This will create a local directory named &lsquo;faiss_index&rsquo; that contains the vector store.</p><h1 id=streamlit-chatbot-interface>StreamLit chatbot interface<a hidden class=anchor aria-hidden=true href=#streamlit-chatbot-interface>#</a></h1><p>Now that we have our index, let&rsquo;s explore how a RAG chatbot would work: When a user interacts with the bot, their query is first processed by a semantic search algorithm to find the most relevant documents from the index. These documents are then fed into a generation model along with the user&rsquo;s query to produce a response.</p><p>I&rsquo;ve wrapped this flow into a StreamLit interface. This is adapted from a StreamLit tutorial on writing an LLM bot<sup id=fnref1:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>, mostly to use Ollama. I&rsquo;ve also added memory to the chatbot, so it will remember the line of questioning. I&rsquo;m including all of the code here, as it&rsquo;s short and straightforward. Let&rsquo;s engage in conversation with our RAG-assisted chatbot using the index we built earlier:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python3</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.chat_models <span style=color:#f92672>import</span> ChatOllama
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.embeddings <span style=color:#f92672>import</span> OllamaEmbeddings
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.prompts <span style=color:#f92672>import</span> PromptTemplate
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_core.output_parsers <span style=color:#f92672>import</span> StrOutputParser
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_core.prompts <span style=color:#f92672>import</span> ChatPromptTemplate
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.schema <span style=color:#f92672>import</span> format_document
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_core.messages <span style=color:#f92672>import</span> get_buffer_string
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_core.runnables <span style=color:#f92672>import</span> RunnableLambda, RunnablePassthrough
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.vectorstores <span style=color:#f92672>import</span> FAISS
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> operator <span style=color:#f92672>import</span> itemgetter
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.memory <span style=color:#f92672>import</span> ConversationBufferMemory
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> streamlit <span style=color:#66d9ef>as</span> st
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>vectorstore <span style=color:#f92672>=</span> FAISS<span style=color:#f92672>.</span>load_local(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;faiss_index&#34;</span>, embeddings<span style=color:#f92672>=</span>OllamaEmbeddings())
</span></span><span style=display:flex><span>retriever <span style=color:#f92672>=</span> vectorstore<span style=color:#f92672>.</span>as_retriever()
</span></span><span style=display:flex><span>chat <span style=color:#f92672>=</span> ChatOllama(model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;mistral&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>memory <span style=color:#f92672>=</span> ConversationBufferMemory(
</span></span><span style=display:flex><span>    return_messages<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, output_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;answer&#34;</span>, input_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;question&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#75715e># First we add a step to load memory</span>
</span></span><span style=display:flex><span><span style=color:#75715e># This adds a &#34;memory&#34; key to the input object</span>
</span></span><span style=display:flex><span>loaded_memory <span style=color:#f92672>=</span> RunnablePassthrough<span style=color:#f92672>.</span>assign(
</span></span><span style=display:flex><span>    chat_history<span style=color:#f92672>=</span>RunnableLambda(
</span></span><span style=display:flex><span>        memory<span style=color:#f92672>.</span>load_memory_variables) <span style=color:#f92672>|</span> itemgetter(<span style=color:#e6db74>&#34;history&#34;</span>),
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>DEFAULT_DOCUMENT_PROMPT <span style=color:#f92672>=</span> PromptTemplate<span style=color:#f92672>.</span>from_template(
</span></span><span style=display:flex><span>    template<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{page_content}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_combine_documents</span>(
</span></span><span style=display:flex><span>    docs, document_prompt<span style=color:#f92672>=</span>DEFAULT_DOCUMENT_PROMPT, document_separator<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>):
</span></span><span style=display:flex><span>    doc_strings <span style=color:#f92672>=</span> [format_document(doc, document_prompt) <span style=color:#66d9ef>for</span> doc <span style=color:#f92672>in</span> docs]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> document_separator<span style=color:#f92672>.</span>join(doc_strings)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>_template <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;&#34;Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>Chat History:
</span></span></span><span style=display:flex><span><span style=color:#e6db74></span><span style=color:#e6db74>{chat_history}</span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>Follow Up Input: </span><span style=color:#e6db74>{question}</span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>Standalone question:&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>CONDENSE_QUESTION_PROMPT <span style=color:#f92672>=</span> PromptTemplate<span style=color:#f92672>.</span>from_template(_template)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>template <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;&#34;Answer the question based only on the following context:
</span></span></span><span style=display:flex><span><span style=color:#e6db74></span><span style=color:#e6db74>{context}</span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>Question: </span><span style=color:#e6db74>{question}</span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>ANSWER_PROMPT <span style=color:#f92672>=</span> ChatPromptTemplate<span style=color:#f92672>.</span>from_template(template)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Now we calculate the standalone question</span>
</span></span><span style=display:flex><span>standalone_question <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;standalone_question&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;question&#34;</span>: <span style=color:#66d9ef>lambda</span> x: x[<span style=color:#e6db74>&#34;question&#34;</span>],
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;chat_history&#34;</span>: <span style=color:#66d9ef>lambda</span> x: get_buffer_string(x[<span style=color:#e6db74>&#34;chat_history&#34;</span>]),
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#f92672>|</span> CONDENSE_QUESTION_PROMPT
</span></span><span style=display:flex><span>    <span style=color:#f92672>|</span> chat
</span></span><span style=display:flex><span>    <span style=color:#f92672>|</span> StrOutputParser(),
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Now we retrieve the documents</span>
</span></span><span style=display:flex><span>retrieved_documents <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;docs&#34;</span>: itemgetter(<span style=color:#e6db74>&#34;standalone_question&#34;</span>) <span style=color:#f92672>|</span> retriever,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;question&#34;</span>: <span style=color:#66d9ef>lambda</span> x: x[<span style=color:#e6db74>&#34;standalone_question&#34;</span>],
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span><span style=color:#75715e># Now we construct the inputs for the final prompt</span>
</span></span><span style=display:flex><span>final_inputs <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;context&#34;</span>: <span style=color:#66d9ef>lambda</span> x: _combine_documents(x[<span style=color:#e6db74>&#34;docs&#34;</span>]),
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;question&#34;</span>: itemgetter(<span style=color:#e6db74>&#34;question&#34;</span>),
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span><span style=color:#75715e># And finally, we do the part that returns the answers</span>
</span></span><span style=display:flex><span>answer <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;answer&#34;</span>: final_inputs <span style=color:#f92672>|</span> ANSWER_PROMPT <span style=color:#f92672>|</span> chat,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;docs&#34;</span>: itemgetter(<span style=color:#e6db74>&#34;docs&#34;</span>),
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span><span style=color:#75715e># And now we put it all together!</span>
</span></span><span style=display:flex><span>final_chain <span style=color:#f92672>=</span> loaded_memory <span style=color:#f92672>|</span> standalone_question <span style=color:#f92672>|</span> retrieved_documents <span style=color:#f92672>|</span> answer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>st<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;NoteChat&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># Set a default model</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;model&#34;</span> <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> st<span style=color:#f92672>.</span>session_state:
</span></span><span style=display:flex><span>    st<span style=color:#f92672>.</span>session_state[<span style=color:#e6db74>&#34;model&#34;</span>] <span style=color:#f92672>=</span> final_chain
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Initialize chat history</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;memory&#34;</span> <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> st<span style=color:#f92672>.</span>session_state:
</span></span><span style=display:flex><span>    st<span style=color:#f92672>.</span>session_state[<span style=color:#e6db74>&#34;memory&#34;</span>] <span style=color:#f92672>=</span> memory
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;messages&#34;</span> <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> st<span style=color:#f92672>.</span>session_state:
</span></span><span style=display:flex><span>    st<span style=color:#f92672>.</span>session_state[<span style=color:#e6db74>&#34;messages&#34;</span>] <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Display chat messages from history on app rerun</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> message <span style=color:#f92672>in</span> st<span style=color:#f92672>.</span>session_state<span style=color:#f92672>.</span>messages:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> st<span style=color:#f92672>.</span>chat_message(message[<span style=color:#e6db74>&#34;role&#34;</span>]):
</span></span><span style=display:flex><span>        st<span style=color:#f92672>.</span>markdown(message[<span style=color:#e6db74>&#34;content&#34;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Accept user input</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> prompt <span style=color:#f92672>:=</span> st<span style=color:#f92672>.</span>chat_input(<span style=color:#e6db74>&#34;What is up?&#34;</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Add user message to chat history</span>
</span></span><span style=display:flex><span>    st<span style=color:#f92672>.</span>session_state<span style=color:#f92672>.</span>messages<span style=color:#f92672>.</span>append({<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: prompt})
</span></span><span style=display:flex><span>    <span style=color:#75715e># Display user message in chat message container</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> st<span style=color:#f92672>.</span>chat_message(<span style=color:#e6db74>&#34;user&#34;</span>):
</span></span><span style=display:flex><span>        st<span style=color:#f92672>.</span>markdown(prompt)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Display assistant response in chat message container</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> st<span style=color:#f92672>.</span>chat_message(<span style=color:#e6db74>&#34;assistant&#34;</span>):
</span></span><span style=display:flex><span>        message_placeholder <span style=color:#f92672>=</span> st<span style=color:#f92672>.</span>empty()
</span></span><span style=display:flex><span>        full_response <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>    response <span style=color:#f92672>=</span> final_chain<span style=color:#f92672>.</span>invoke({<span style=color:#e6db74>&#34;question&#34;</span>: prompt})
</span></span><span style=display:flex><span>    memory<span style=color:#f92672>.</span>save_context({<span style=color:#e6db74>&#34;question&#34;</span>: prompt}, {
</span></span><span style=display:flex><span>                        <span style=color:#e6db74>&#34;answer&#34;</span>: response[<span style=color:#e6db74>&#34;answer&#34;</span>]<span style=color:#f92672>.</span>content})
</span></span><span style=display:flex><span>    full_response <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;&#34;&#34;</span><span style=color:#e6db74>{</span>response[<span style=color:#e6db74>&#39;answer&#39;</span>]<span style=color:#f92672>.</span>content<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># for doc in response[&#34;answer&#34;][&#34;doc&#34;]:</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#     full_response += f&#34;\n- {doc[&#39;path&#39;]}&#34;</span>
</span></span><span style=display:flex><span>    full_response <span style=color:#f92672>+=</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>    st<span style=color:#f92672>.</span>session_state<span style=color:#f92672>.</span>messages<span style=color:#f92672>.</span>append(
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;assistant&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: full_response})
</span></span></code></pre></div><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>The combination of RAG and semantic search using vector-based indexing allows conversational AI systems to deliver more accurate and comprehensive responses. With open-source tools like LangChain and Ollama, developers can easily experiment with these technologies and push the boundaries of conversational AI applications.</p><p>As a preliminary qualitative evaluation, it has been interesting to explore how an LLM &lsquo;understands&rsquo; my notes. I&rsquo;m impressed by how it will draw in contrasting ideas I wouldn&rsquo;t have thought of, but it also has issues, it frequently gets confused and doesn&rsquo;t respond as I would expect, potentially because the corpus of my notes is very diverse, it includes highlights on anything I have read. I&rsquo;ve seen the LLM respond as a medieval lord, because it pulled in a note from a fantasy book when I asked a philosophical question. Further work is needed to tune the prompt I&rsquo;m using. I&rsquo;d also like to find ways to do more quantitative evaluation of the results returned, even though this is only a toy side project.</p><h3 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h3><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Ollama is a great open-source local LLM server <a href=https://ollama.com>https://ollama.com</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>LangChain is an open-source framework designed for developing applications using LLMs. <a href=https://langchain.ai>https://langchain.ai</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>StreamLit is a powerful way to turn data scripts into shareable web apps <a href=https://streamlit.io>https://streamlit.io</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p><a href=https://www.elastic.co/elasticsearch>https://www.elastic.co/elasticsearch</a>, or the open alternative <a href=https://opensearch.org>https://opensearch.org</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p><a href=https://github.com/spotify/annoy>https://github.com/spotify/annoy</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.eoinhurrell.com/tags/post/>Post</a></li></ul><nav class=paginav><a class=prev href=https://www.eoinhurrell.com/posts/20240425-data-enrichment-llm-tags/><span class=title>« Prev</span><br><span>Data Enrichment with LLMs for Obsidian: Better Tags</span>
</a><a class=next href=https://www.eoinhurrell.com/posts/weeknotes_week_9_2023/><span class=title>Next »</span><br><span>Weeknotes - Week 9 - 2023</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Retrieval-Augmented Generation Experiments (Part 1) on x" href="https://x.com/intent/tweet/?text=Retrieval-Augmented%20Generation%20Experiments%20%28Part%201%29&amp;url=https%3a%2f%2fwww.eoinhurrell.com%2fposts%2f20240214-rag-blog-part-1-how%2f&amp;hashtags=post"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Retrieval-Augmented Generation Experiments (Part 1) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.eoinhurrell.com%2fposts%2f20240214-rag-blog-part-1-how%2f&amp;title=Retrieval-Augmented%20Generation%20Experiments%20%28Part%201%29&amp;summary=Retrieval-Augmented%20Generation%20Experiments%20%28Part%201%29&amp;source=https%3a%2f%2fwww.eoinhurrell.com%2fposts%2f20240214-rag-blog-part-1-how%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Retrieval-Augmented Generation Experiments (Part 1) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fwww.eoinhurrell.com%2fposts%2f20240214-rag-blog-part-1-how%2f&title=Retrieval-Augmented%20Generation%20Experiments%20%28Part%201%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Retrieval-Augmented Generation Experiments (Part 1) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.eoinhurrell.com%2fposts%2f20240214-rag-blog-part-1-how%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Retrieval-Augmented Generation Experiments (Part 1) on whatsapp" href="https://api.whatsapp.com/send?text=Retrieval-Augmented%20Generation%20Experiments%20%28Part%201%29%20-%20https%3a%2f%2fwww.eoinhurrell.com%2fposts%2f20240214-rag-blog-part-1-how%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Retrieval-Augmented Generation Experiments (Part 1) on telegram" href="https://telegram.me/share/url?text=Retrieval-Augmented%20Generation%20Experiments%20%28Part%201%29&amp;url=https%3a%2f%2fwww.eoinhurrell.com%2fposts%2f20240214-rag-blog-part-1-how%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Retrieval-Augmented Generation Experiments (Part 1) on ycombinator" href="https://news.ycombinator.com/submitlink?t=Retrieval-Augmented%20Generation%20Experiments%20%28Part%201%29&u=https%3a%2f%2fwww.eoinhurrell.com%2fposts%2f20240214-rag-blog-part-1-how%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://www.eoinhurrell.com/></a></span>·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>